{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSSM深度召回实践\n",
    "关于DSSM的论文和方案非常的多，简单来说就是构建一个user的embedding和item的embedding然后进行匹配，通过优化匹配的距离来达到让用户感兴趣的item跟用户的相似度更高，用户不感兴趣的item跟用户相似度更低的目的。模型的结构看起来非常的简单，但是要能做出效果来可能还需要一定的经验和技巧，很多时候拿一个数据集进行实验，效果可能是非常非常差的，模型基本没有区分能力，跟itemCF完全没有对比性。这里对DSSM方案进行实践，通过经典的方案和一些优化技巧，来让DSSM达到一定的可用效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dnn_model import DNN\n",
    "from encoder_model import Encoder\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from weight_initiallizer import Initializer\n",
    "import torch.nn.init as init\n",
    "from itemcf import itemcf_sim\n",
    "import time\n",
    "import os\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "本次实验选用的是movielen 1M的数据集，实验对比方案为itemCF，数据切分按照时间以大于2003-01-01的数据为验证集，小于2020-01-01的数据集为训练集，同时对于验证集的用户要保证其在训练集中有历史行为，因此需要过滤掉在训练集中没有行为的用户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-cf8b99496bf9>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  user = pd.read_csv(os.path.join(root_path, 'ml-1m', 'users.dat'), sep='::', names = ['user', 'gender', 'age',\n",
      "<ipython-input-2-cf8b99496bf9>:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  movie = pd.read_csv(os.path.join(root_path, 'ml-1m', 'movies.dat'), sep='::', names = ['movie', 'title', 'genres'])\n",
      "<ipython-input-2-cf8b99496bf9>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  rating = pd.read_csv(os.path.join(root_path, 'ml-1m', 'ratings.dat'), sep='::', names = ['user', 'movie', 'ratings',\n"
     ]
    }
   ],
   "source": [
    "root_path = os.path.abspath('.')\n",
    "user = pd.read_csv(os.path.join(root_path, 'ml-1m', 'users.dat'), sep='::', names = ['user', 'gender', 'age', \n",
    "                                                                                     'occupation', 'zip_code'])\n",
    "movie = pd.read_csv(os.path.join(root_path, 'ml-1m', 'movies.dat'), sep='::', names = ['movie', 'title', 'genres'])\n",
    "rating = pd.read_csv(os.path.join(root_path, 'ml-1m', 'ratings.dat'), sep='::', names = ['user', 'movie', 'ratings', \n",
    "                                                                                     'timestamp'])\n",
    "# mapping\n",
    "user_id_dict = dict()\n",
    "for idx, uid in enumerate(user['user'].tolist()):\n",
    "    user_id_dict[uid] = idx\n",
    "movie_id_dict = dict()\n",
    "for idx, mid in enumerate(movie['movie'].tolist()):\n",
    "    movie_id_dict[mid] = idx\n",
    "user['user'] = user['user'].map(user_id_dict)\n",
    "movie['movie'] = movie['movie'].map(movie_id_dict)\n",
    "rating['user'] = rating['user'].map(user_id_dict)\n",
    "rating['movie'] = rating['movie'].map(movie_id_dict)\n",
    "\n",
    "\n",
    "# 时间处理\n",
    "rating['timestamp'] = rating['timestamp'].apply(lambda x: time.localtime(x))\n",
    "rating['time_str'] = rating['timestamp'].apply(lambda x: \n",
    "                                                  time.strftime(\"%Y-%m-%d %H:%M:%S\",x))\n",
    "\n",
    "# 切分训练和验证集，数据是2000到2003年的，以2003年用户的行为为验证集，用用户2000-2002年数据进行预测和验证，过滤掉那些只在2003年有行为的用户\n",
    "train_data = rating[rating['time_str']<'2003-01-01 00:00:00']\n",
    "val_data = rating[rating['time_str']>='2003-01-01 00:00:00']\n",
    "# 过滤\n",
    "val_data = val_data[val_data['user'].isin(train_data['user'].unique())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于ItemCF的base方案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996861/996861 [01:47<00:00, 9279.35it/s]\n",
      "100%|██████████| 6040/6040 [18:58<00:00,  5.31it/s]  \n",
      "100%|██████████| 3348/3348 [00:00<00:00, 7867.68it/s]\n",
      "100%|██████████| 178/178 [00:10<00:00, 16.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall is 0.057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# itemcf\n",
    "user_movie_dict = dict()\n",
    "for idx, rows in tqdm(train_data.iterrows(), total=len(train_data)):\n",
    "    u = rows['user']\n",
    "    m = rows['movie']\n",
    "    dt = rows['time_str']\n",
    "    if u not in user_movie_dict:\n",
    "        user_movie_dict[u] = [(m, dt)]\n",
    "    else:\n",
    "         user_movie_dict[u].append((m, dt))\n",
    "            \n",
    "# 计算movie之间的相似度\n",
    "sim = itemcf_sim(user_movie_dict)\n",
    "\n",
    "user_movie_dict_val = dict()\n",
    "for idx, rows in tqdm(val_data.iterrows(), total=len(val_data)):\n",
    "    u = rows['user']\n",
    "    m = rows['movie']\n",
    "    if u not in user_movie_dict_val:\n",
    "        user_movie_dict_val[u] = [m]\n",
    "    else:\n",
    "         user_movie_dict_val[u].append(m)\n",
    "            \n",
    "            \n",
    "user_rec_dict = dict()\n",
    "for uid, _ in tqdm(user_movie_dict_val.items()):\n",
    "    movies = user_movie_dict[uid]\n",
    "    user_rec_dict[uid] = dict()\n",
    "    for mov,_ in movies:\n",
    "        for m, w in sorted(sim[mov].items(), key=lambda x:x[1], reverse=True)[:50]:\n",
    "            if m not in movies:\n",
    "                if m not in user_rec_dict[uid]:\n",
    "                    user_rec_dict[uid][m] = w\n",
    "                else:\n",
    "                    user_rec_dict[uid][m] += w\n",
    "\n",
    "# 计算召回\n",
    "hits, total = 0, 0\n",
    "for uid, movies in user_rec_dict.items():\n",
    "    rec_movies = [m for m, _ in sorted(movies.items(), key=lambda x: x[1], reverse=True)[:50]]\n",
    "    hits += len(set(rec_movies) & set(user_movie_dict_val[uid]))\n",
    "    total += len(user_movie_dict_val[uid])\n",
    "# itemcf recall = 0.0570\n",
    "print(\"recall is %.3f\" % (hits/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于0，1的正负样本构造方案\n",
    "### 数据集构造\n",
    "方案1:将用户评分的电影作为正样本，在所有的电影中随机采样若干个电影作为用户的负样本；\n",
    "方案2:对数据集中用户评分较高的作为正样本，用户评分低的作为负样本；\n",
    "方案1和2其实都有可用的场景，当我们面对不同的任务的时候可以采样不同的数据构造方案，我们需要清楚的意识到模型是死的，但是数据是活的，同一个模型输入不同的数据，模型能学到的东西是完全不一样的，因此对于不同的任务我们需要对任务的目标构造出适合任务的数据集。这里我们的任务是召回，也就是从全量电影资源中选择用户感兴趣的电影，面对的是整个电影数据资源，感兴趣的对用户来说是一个较为模糊的结果，对精度的要求没有那么高。在来看方案1和2显然方案2所面对的资源是全量电影资源中很少一部分，对于大多数电影是没有预测能力的，同时由于对电影的评分进行了细粒度的量化，模型能很好的区分哪些电影是用户喜欢的哪些是不喜欢的精度较高，显然方案2更适合排序。方案1的数据是全量电影数据，对于正负样本的定义也没那么精准，所以模型学习到的也是一个较为模糊的偏好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机负采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996861/996861 [08:58<00:00, 1852.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# 随机负采样\n",
    "sample_list = list(train_data['movie'].unique())\n",
    "data = list()\n",
    "for idx, rows in tqdm(train_data.iterrows(), total=len(train_data)):\n",
    "    use = rows['user']\n",
    "    mov = rows['movie']\n",
    "    data.append([use, mov, 1])\n",
    "    for m in np.random.choice(sample_list, 3):\n",
    "        data.append([use, m, 0])\n",
    "data = pd.DataFrame(data, columns=['user', 'movie', 'tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 带权负采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加权负采样(跟上面的随机负采样只能选择一种)\n",
    "tmp = train_data['movie'].value_counts().reset_index(name='count')\n",
    "high_frequency_df = tmp[tmp['count']>=5]\n",
    "high_frequency = high_frequency_df['index'].tolist()\n",
    "high_frequency_p = list()\n",
    "for m, c in zip(high_frequency, high_frequency_df['count']):\n",
    "    high_frequency_p.append(c**(3/4))\n",
    "hig_frequency_p = [c/sum(high_frequency_p) for c in high_frequency_p]\n",
    "low_frequency = tmp[tmp['count']<5]['index'].tolist()\n",
    "data = list()\n",
    "for idx, rows in tqdm(train_data.iterrows(), total=len(train_data)):\n",
    "    use = rows['user']\n",
    "    mov = rows['movie']\n",
    "    data.append([use, mov, 1])\n",
    "    for i in range(3):\n",
    "        if np.random.random() > 0.7:\n",
    "            mov = np.random.choice(high_frequency, p=hig_frequency_p, size=1)[0]\n",
    "            data.append([use, mov, 0])\n",
    "        else:\n",
    "            mov = np.random.choice(low_frequency, size=1)[0]\n",
    "            data.append([use, mov, 0])\n",
    "data = pd.DataFrame(data, columns=['user', 'movie', 'tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练数据构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.x = data[0]\n",
    "        self.y = data[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "        data = (x, y)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "# 训练集验证集随机分割\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=2021)\n",
    "train_x = train_df[['user', 'movie']].values\n",
    "train_y = train_df['tag'].values\n",
    "test_x = test_df[['user', 'movie']].values\n",
    "test_y = test_df['tag'].values\n",
    "\n",
    "# 构造dataloader\n",
    "train_dataset = trainset((train_x, train_y))\n",
    "test_dataset = trainset((test_x, test_y))\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNModel(nn.Module):\n",
    "    def __init__(self, inp_user, inp_movie, out, input_user_categorical_feature, input_movie_categorical_feature, \n",
    "                 hidden_layers, dropouts, batch_norm):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.user_embed = nn.Embedding(input_user_categorical_feature[0][0], input_user_categorical_feature[0][1])\n",
    "        self.movie_embed = nn.Embedding(input_movie_categorical_feature[0][0], input_movie_categorical_feature[0][1])\n",
    "        \n",
    "        self.user_dnn = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.movie_dnn = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        u = self.user_embed(x[:, 0])\n",
    "        m = self.movie_embed(x[:, 1])\n",
    "        u = self.user_dnn(u)\n",
    "        m = self.movie_dnn(m)\n",
    "        u = u/torch.sum(u*u, 1).view(-1,1)\n",
    "        m = m/torch.sum(m*m, 1).view(-1,1)\n",
    "        return u, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epoch, loss_function, optimizer, path, early_stop):\n",
    "    \"\"\"\n",
    "    pytorch 模型训练通用代码\n",
    "    :param model: pytorch 模型\n",
    "    :param train_loader: dataloader, 训练数据\n",
    "    :param val_loader: dataloader, 验证数据\n",
    "    :param epoch: int, 训练迭代次数\n",
    "    :param loss_function: 优化损失函数\n",
    "    :param optimizer: pytorch优化器\n",
    "    :param path: save path\n",
    "    :param early_stop: int, 提前停止步数\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # 是否使用GPU\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 多少步内验证集的loss没有变小就提前停止\n",
    "    patience, eval_loss = 0, 0\n",
    "    \n",
    "    # 训练\n",
    "    for i in range(epoch):\n",
    "        total_loss, count = 0, 0\n",
    "        y_pred = list()\n",
    "        y_true = list()\n",
    "        for idx, (x, y) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            x, y = x.to(device), y.to(device) \n",
    "            u, m = model(x)\n",
    "            predict = torch.sigmoid(torch.sum(u*m, 1))\n",
    "            y_pred.extend(predict.cpu().detach().numpy())\n",
    "            y_true.extend(y.cpu().detach().numpy())\n",
    "            loss = loss_function(predict, y.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss)\n",
    "            count += 1\n",
    "            \n",
    "        train_auc = roc_auc_score(np.array(y_true), np.array(y_pred))\n",
    "        torch.save(model, path.format(i+1))\n",
    "        print(\"Epoch %d train loss is %.3f and train auc is %.3f\" % (i+1, total_loss / count, train_auc))\n",
    "    \n",
    "        # 验证\n",
    "        total_eval_loss = 0\n",
    "        model.eval()\n",
    "        count_eval = 0\n",
    "        val_y_pred = list()\n",
    "        val_true = list()\n",
    "        for idx, (x, y) in tqdm(enumerate(val_loader), total=len(val_loader)):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            u, m = model(x)\n",
    "            predict = torch.sigmoid(torch.sum(u*m, 1))\n",
    "            val_y_pred.extend(predict.cpu().detach().numpy())\n",
    "            val_true.extend(y.cpu().detach().numpy())\n",
    "            loss = loss_function(predict, y.float())\n",
    "            total_eval_loss += float(loss)\n",
    "            count_eval += 1\n",
    "        val_auc = roc_auc_score(np.array(y_true), np.array(y_pred))\n",
    "        print(\"Epoch %d val loss is %.3fand train auc is %.3f\" % (i+1, total_eval_loss / count_eval, val_auc))\n",
    "        \n",
    "        # 提前停止策略\n",
    "        if i == 0:\n",
    "            eval_loss = total_eval_loss / count_eval\n",
    "        else:\n",
    "            if total_eval_loss / count_eval < eval_loss:\n",
    "                eval_loss = total_eval_loss / count_eval\n",
    "            else:\n",
    "                if patience < early_stop:\n",
    "                    patience += 1\n",
    "                else:\n",
    "                    print(\"val loss is not decrease in %d epoch and break training\" % patience)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data_share/movie_len/weight_initiallizer.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  initialization(m.weight.data, **kwargs)\n",
      "/data_share/movie_len/weight_initiallizer.py:22: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  initialization(m.bias.data)\n",
      "100%|██████████| 24922/24922 [02:39<00:00, 156.07it/s]\n",
      "  1%|          | 34/6231 [00:00<00:18, 331.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss is 0.691 and train auc is 0.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:17<00:00, 351.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 val loss is 0.689and train auc is 0.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24922/24922 [02:40<00:00, 155.62it/s]\n",
      "  0%|          | 18/6231 [00:00<00:37, 167.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss is 0.689 and train auc is 0.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:19<00:00, 319.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 val loss is 0.689and train auc is 0.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24922/24922 [02:39<00:00, 156.43it/s]\n",
      "  1%|          | 34/6231 [00:00<00:18, 333.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train loss is 0.689 and train auc is 0.666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:17<00:00, 355.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 val loss is 0.689and train auc is 0.666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24922/24922 [02:39<00:00, 156.52it/s]\n",
      "  1%|          | 44/6231 [00:00<00:14, 435.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train loss is 0.689 and train auc is 0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:17<00:00, 363.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 val loss is 0.689and train auc is 0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24922/24922 [02:40<00:00, 155.16it/s]\n",
      "  1%|          | 33/6231 [00:00<00:19, 325.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train loss is 0.688 and train auc is 0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:17<00:00, 350.48it/s]\n",
      "  0%|          | 0/24922 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 val loss is 0.689and train auc is 0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24922/24922 [02:38<00:00, 157.30it/s]\n",
      "  1%|          | 43/6231 [00:00<00:14, 415.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train loss is 0.688 and train auc is 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:17<00:00, 359.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 val loss is 0.688and train auc is 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24922/24922 [02:41<00:00, 154.08it/s]\n",
      "  1%|          | 44/6231 [00:00<00:14, 434.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train loss is 0.688 and train auc is 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:17<00:00, 360.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 val loss is 0.689and train auc is 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24922/24922 [02:48<00:00, 148.34it/s]\n",
      "  0%|          | 12/6231 [00:00<00:55, 112.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train loss is 0.688 and train auc is 0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:18<00:00, 341.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 val loss is 0.689and train auc is 0.665\n",
      "val loss is not decrease in 3 epoch and break training\n"
     ]
    }
   ],
   "source": [
    "# 模型初始化\n",
    "inp_user = 128\n",
    "inp_movie = 128\n",
    "out = 64\n",
    "input_user_categorical_feature = {0: (6040, 128)}\n",
    "input_movie_categorical_feature =  {0: (3883, 128)}\n",
    "hidden_layers = [128, 64]\n",
    "dropouts = [0.5, 0.5, 0.5]\n",
    "batch_norm = False\n",
    "\n",
    "model = DNNModel(inp_user, inp_movie, out, input_user_categorical_feature, input_movie_categorical_feature, \n",
    "                 hidden_layers, dropouts, batch_norm)\n",
    "Initializer.initialize(model=model, initialization=init.xavier_uniform, gain=init.calculate_gain('relu'))\n",
    "# 模型训练\n",
    "epoch = 20\n",
    "loss_function = F.binary_cross_entropy_with_logits\n",
    "early_stop = 3\n",
    "learn_rate = 0.005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "path = 'model/model_{}.pth'\n",
    "\n",
    "train_model(model, train_loader, test_loader, epoch, loss_function, optimizer, path, early_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3348/3348 [00:00<00:00, 9357.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall is 0.071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 结果验证\n",
    "model.eval()\n",
    "user['movie'] = 1\n",
    "test_x = user[['user', 'movie']].values\n",
    "x = torch.from_numpy(test_x).cuda()\n",
    "user_embed, _ = model(x)\n",
    "\n",
    "movie['user'] = 1\n",
    "test_x = movie[['user', 'movie']].values\n",
    "x = torch.from_numpy(test_x).cuda()\n",
    "_, movie_embed = model(x)\n",
    "\n",
    "movie_embed = movie_embed.cpu().detach().numpy()\n",
    "user_embed = user_embed.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# faiss索引构建\n",
    "d = 64\n",
    "nlist = 10\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(movie_embed)\n",
    "\n",
    "# 验证集数据字典化\n",
    "user_movie_dict_val = dict()\n",
    "for idx, rows in tqdm(val_data.iterrows(), total=len(val_data)):\n",
    "    u = rows['user']\n",
    "    m = rows['movie']\n",
    "    if u not in user_movie_dict_val:\n",
    "        user_movie_dict_val[u] = [m]\n",
    "    else:\n",
    "         user_movie_dict_val[u].append(m)\n",
    "            \n",
    "# 用户推荐结果索引           \n",
    "D, I = index.search(user_embed[list(val_data['user'].unique())], 50)\n",
    "\n",
    "# 召回率计算\n",
    "hits, total = 0, 0\n",
    "for uid, rec_list in zip(list(val_data['user'].unique()), I):\n",
    "    hits += len(set(rec_list)&set(user_movie_dict_val[uid]))\n",
    "    total += len(user_movie_dict_val[uid])\n",
    "print(\"recall is %.3f\" % (hits/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于正负样本距离的损失函数构造方案\n",
    "上面构造数据集的方案有一个很严重的缺陷，因为构造的都是0，1样本这种样本属于hard级别，对样本的要求非常高而且模型学习起来的难度也非常的大，如果样本没有构造好或者模型参数数据量不够，很难达到理想的效果。因此这里换用一种soft的样本构造方案，即基于triplet loss的方案，triplet loss是保证user向量跟正样本movie向量的距离比负样本movie向量的距离更近一些，loss优化的就是两者之间的距离最小：\n",
    "$$loss = max(d(u,m_p)-d(u,m_n)+margin, 0)$$\n",
    "DSSM方案是选取一个正样本和若干负样本，保证正样本在其中的概率最大:\n",
    "$$P(u|m) = \\frac{exp(\\gamma R(u,m))}{exp(\\gamma R(u,m)) + \\sum{m \\in D^{-}}{-exp(\\gamma R(u,m))}}$$\n",
    "$$loss = -\\sum_{i=1}^{n}{log(P(u|m))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### triplet loss 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996861/996861 [08:57<00:00, 1853.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# triplet loss负采样\n",
    "sample_list = list(train_data['movie'].unique())\n",
    "data = list()\n",
    "for idx, rows in tqdm(train_data.iterrows(), total=len(train_data)):\n",
    "    use = rows['user']\n",
    "    mov = rows['movie']\n",
    "    for m in np.random.choice(sample_list, 3):\n",
    "        data.append([use, mov, m])\n",
    "data = pd.DataFrame(data, columns=['user', 'movie_pos', 'movie_neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.x = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=2021)\n",
    "train_x = train_df[['user', 'movie_pos', 'movie_neg']].values\n",
    "test_x = test_df[['user', 'movie_pos', 'movie_neg']].values\n",
    "\n",
    "train_dataset = trainset((train_x))\n",
    "test_dataset = trainset((test_x))\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplet loss DNNmodel\n",
    "class DNNModel(nn.Module):\n",
    "    def __init__(self, inp_user, inp_movie, out, input_user_categorical_feature, input_movie_categorical_feature, \n",
    "                 hidden_layers, dropouts, batch_norm):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.user_embed = nn.Embedding(input_user_categorical_feature[0][0], input_user_categorical_feature[0][1])\n",
    "        self.movie_embed = nn.Embedding(input_movie_categorical_feature[0][0], input_movie_categorical_feature[0][1])\n",
    "        \n",
    "        self.user_dnn = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.movie_dnn = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        user = self.user_embed(x[:, 0])\n",
    "        movie_pos = self.movie_embed(x[:, 1])\n",
    "        movie_neg = self.movie_embed(x[:, 2])\n",
    "        user = self.user_dnn(user)\n",
    "        movie_pos = self.movie_dnn(movie_pos)\n",
    "        movie_neg = self.movie_dnn(movie_neg)\n",
    "        user = user/torch.sum(user*user, 1).view(-1,1)\n",
    "        movie_pos = movie_pos/torch.sum(movie_pos*movie_pos, 1).view(-1,1)\n",
    "        movie_neg = movie_neg/torch.sum(movie_neg*movie_neg, 1).view(-1,1)\n",
    "        return user, movie_pos, movie_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplet DNN model train\n",
    "def train_model(model, train_loader, val_loader, epoch, loss_function, optimizer, path, early_stop):\n",
    "    \"\"\"\n",
    "    pytorch 模型训练通用代码\n",
    "    :param model: pytorch 模型\n",
    "    :param train_loader: dataloader, 训练数据\n",
    "    :param val_loader: dataloader, 验证数据\n",
    "    :param epoch: int, 训练迭代次数\n",
    "    :param loss_function: 优化损失函数\n",
    "    :param optimizer: pytorch优化器\n",
    "    :param path: save path\n",
    "    :param early_stop: int, 提前停止步数\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # 是否使用GPU\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 多少步内验证集的loss没有变小就提前停止\n",
    "    patience, eval_loss = 0, 0\n",
    "    \n",
    "    # 训练\n",
    "    for i in range(epoch):\n",
    "        total_loss, count = 0, 0\n",
    "        for idx, x in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            x = x.to(device)\n",
    "            u, m_p, m_n = model(x)\n",
    "            loss = loss_function(u, m_p, m_n)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss)\n",
    "            count += 1\n",
    "            \n",
    "#         train_auc = roc_auc_score(np.array(y_true), np.array(y_pred))\n",
    "        torch.save(model, path.format(i+1))\n",
    "        print(\"Epoch %d train loss is %.3f\" % (i+1, total_loss / count))\n",
    "    \n",
    "        # 验证\n",
    "        total_eval_loss = 0\n",
    "        model.eval()\n",
    "        count_eval = 0\n",
    "        for idx, x in tqdm(enumerate(val_loader), total=len(val_loader)):\n",
    "            x = x.to(device)\n",
    "            u, m_p, m_n = model(x)\n",
    "            loss = loss_function(u, m_p, m_n)\n",
    "            total_eval_loss += float(loss)\n",
    "            count_eval += 1\n",
    "        print(\"Epoch %d val loss is %.3f\" % (i+1, total_eval_loss / count_eval))\n",
    "        \n",
    "        # 提前停止策略\n",
    "        if i == 0:\n",
    "            eval_loss = total_eval_loss / count_eval\n",
    "        else:\n",
    "            if total_eval_loss / count_eval < eval_loss:\n",
    "                eval_loss = total_eval_loss / count_eval\n",
    "            else:\n",
    "                if patience < early_stop:\n",
    "                    patience += 1\n",
    "                else:\n",
    "                    print(\"val loss is not decrease in %d epoch and break training\" % patience)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18692/18692 [02:02<00:00, 152.44it/s]\n",
      "  1%|          | 26/4673 [00:00<00:18, 256.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss is 0.506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4673/4673 [00:11<00:00, 417.44it/s]\n",
      "  0%|          | 1/18692 [00:00<57:24,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 val loss is 0.491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18692/18692 [02:01<00:00, 154.33it/s]\n",
      "  1%|          | 44/4673 [00:00<00:10, 439.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss is 0.485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4673/4673 [00:11<00:00, 410.35it/s]\n",
      "  0%|          | 0/18692 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 val loss is 0.485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18692/18692 [02:01<00:00, 154.21it/s]\n",
      "  1%|          | 49/4673 [00:00<00:09, 489.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train loss is 0.483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4673/4673 [00:11<00:00, 405.59it/s]\n",
      "  0%|          | 0/18692 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 val loss is 0.484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18692/18692 [02:01<00:00, 154.21it/s]\n",
      "  1%|          | 37/4673 [00:00<00:12, 362.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train loss is 0.482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4673/4673 [00:11<00:00, 396.37it/s]\n",
      "  0%|          | 1/18692 [00:00<57:31,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 val loss is 0.488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18692/18692 [02:01<00:00, 154.21it/s]\n",
      "  1%|          | 48/4673 [00:00<00:09, 475.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train loss is 0.482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4673/4673 [00:11<00:00, 401.61it/s]\n",
      "  0%|          | 1/18692 [00:00<59:00,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 val loss is 0.487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18692/18692 [02:03<00:00, 151.72it/s]\n",
      "  1%|          | 39/4673 [00:00<00:12, 385.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train loss is 0.482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4673/4673 [00:12<00:00, 388.97it/s]\n",
      "  0%|          | 1/18692 [00:00<46:27,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 val loss is 0.483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18692/18692 [02:02<00:00, 152.80it/s]\n",
      "  1%|          | 48/4673 [00:00<00:09, 478.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train loss is 0.482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4673/4673 [00:11<00:00, 390.17it/s]\n",
      "  0%|          | 1/18692 [00:00<58:05,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 val loss is 0.481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18692/18692 [02:02<00:00, 152.75it/s]\n",
      "  1%|          | 46/4673 [00:00<00:10, 452.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train loss is 0.481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4673/4673 [00:12<00:00, 387.48it/s]\n",
      "  0%|          | 1/18692 [00:00<49:21,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 val loss is 0.486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18692/18692 [02:02<00:00, 152.48it/s]\n",
      "  0%|          | 21/4673 [00:00<00:22, 206.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train loss is 0.482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4673/4673 [00:12<00:00, 381.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 val loss is 0.484\n",
      "val loss is not decrease in 3 epoch and break training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 模型初始化\n",
    "inp_user = 128\n",
    "inp_movie = 128\n",
    "out = 64\n",
    "input_user_categorical_feature = {0: (6040, 128)}\n",
    "input_movie_categorical_feature =  {0: (3883, 128)}\n",
    "hidden_layers = [128, 64]\n",
    "dropouts = [0.5, 0.5, 0.5]\n",
    "batch_norm = False\n",
    "\n",
    "model = DNNModel(inp_user, inp_movie, out, input_user_categorical_feature, input_movie_categorical_feature, \n",
    "                 hidden_layers, dropouts, batch_norm)\n",
    "\n",
    "# 模型训练\n",
    "epoch = 20\n",
    "loss_function = F.triplet_margin_with_distance_loss\n",
    "early_stop = 3\n",
    "learn_rate = 0.005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "path = 'model/model_{}.pth'\n",
    "\n",
    "train_model(model, train_loader, test_loader, epoch, loss_function, optimizer, path, early_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3348/3348 [00:00<00:00, 10039.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall is 0.071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 结果验证\n",
    "model.eval()\n",
    "user['movie_pos'] = 1\n",
    "user['movie_neg'] = 2\n",
    "test_x = user[['user', 'movie_pos', 'movie_neg']].values\n",
    "x = torch.from_numpy(test_x).cuda()\n",
    "user_embed, _, _ = model(x)\n",
    "\n",
    "movie['user'] = 1\n",
    "movie['movie_neg'] = 1\n",
    "test_x = movie[['user', 'movie', 'movie_neg']].values\n",
    "x = torch.from_numpy(test_x).cuda()\n",
    "_, movie_embed, _ = model(x)\n",
    "\n",
    "movie_embed = movie_embed.cpu().detach().numpy()\n",
    "user_embed = user_embed.cpu().detach().numpy()\n",
    "\n",
    "# embedding 维度\n",
    "d = 64\n",
    "nlist = 10\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(movie_embed)\n",
    "\n",
    "user_movie_dict_val = dict()\n",
    "for idx, rows in tqdm(val_data.iterrows(), total=len(val_data)):\n",
    "    u = rows['user']\n",
    "    m = rows['movie']\n",
    "    if u not in user_movie_dict_val:\n",
    "        user_movie_dict_val[u] = [m]\n",
    "    else:\n",
    "         user_movie_dict_val[u].append(m)\n",
    "            \n",
    "D, I = index.search(user_embed[list(val_data['user'].unique())], 50)\n",
    "\n",
    "# 召回率计算\n",
    "hits, total = 0, 0\n",
    "for uid, rec_list in zip(list(val_data['user'].unique()), I):\n",
    "    hits += len(set(rec_list)&set(user_movie_dict_val[uid]))\n",
    "    total += len(user_movie_dict_val[uid])\n",
    "print(\"recall is %.3f\" % (hits/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DSSM loss 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996861/996861 [08:34<00:00, 1936.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# DSSM负采样\n",
    "sample_list = list(train_data['movie'].unique())\n",
    "data = list()\n",
    "for idx, rows in tqdm(train_data.iterrows(), total=len(train_data)):\n",
    "    use = rows['user']\n",
    "    mov = rows['movie']\n",
    "    m = np.random.choice(sample_list, 3)\n",
    "    data.append([use, mov] + list(m))\n",
    "data = pd.DataFrame(data, columns=['user', 'movie_pos', 'movie_neg1', 'movie_neg2', 'movie_neg3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.x = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=2021)\n",
    "train_x = train_df[['user', 'movie_pos', 'movie_neg1', 'movie_neg2', 'movie_neg3']].values\n",
    "test_x = test_df[['user', 'movie_pos', 'movie_neg1', 'movie_neg2', 'movie_neg3']].values\n",
    "\n",
    "train_dataset = trainset((train_x))\n",
    "test_dataset = trainset((test_x))\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSSM DNNmodel\n",
    "class DNNModel(nn.Module):\n",
    "    def __init__(self, inp_user, inp_movie, out, input_user_categorical_feature, input_movie_categorical_feature, \n",
    "                 hidden_layers, dropouts, batch_norm):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.user_embed = nn.Embedding(input_user_categorical_feature[0][0], input_user_categorical_feature[0][1])\n",
    "        self.movie_embed = nn.Embedding(input_movie_categorical_feature[0][0], input_movie_categorical_feature[0][1])\n",
    "        \n",
    "        self.user_dnn = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "#             nn.Dropout(0.5),\n",
    "            nn.Linear(64, 64)\n",
    "        )\n",
    "        \n",
    "        self.movie_dnn = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "#             nn.Dropout(0.5),\n",
    "            nn.Linear(64, 64)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        user = self.user_embed(x[:, 0])\n",
    "        movie_pos = self.movie_embed(x[:, 1])\n",
    "        movie_neg1 = self.movie_embed(x[:, 2])\n",
    "        movie_neg2 = self.movie_embed(x[:, 3])\n",
    "        movie_neg3 = self.movie_embed(x[:, 4])\n",
    "        \n",
    "        user = self.user_dnn(user)\n",
    "        movie_pos = self.movie_dnn(movie_pos)\n",
    "        movie_neg1 = self.movie_dnn(movie_neg1)\n",
    "        movie_neg2 = self.movie_dnn(movie_neg2)\n",
    "        movie_neg3 = self.movie_dnn(movie_neg3)\n",
    "        user = user/torch.sum(user*user, 1).view(-1,1)\n",
    "        movie_pos = movie_pos/torch.sum(movie_pos*movie_pos, 1).view(-1,1)\n",
    "        movie_neg1 = movie_neg1/torch.sum(movie_neg1*movie_neg1, 1).view(-1,1)\n",
    "        movie_neg2 = movie_neg2/torch.sum(movie_neg2*movie_neg2, 1).view(-1,1)\n",
    "        movie_neg3 = movie_neg3/torch.sum(movie_neg3*movie_neg3, 1).view(-1,1)\n",
    "        return user, movie_pos, movie_neg1,  movie_neg2, movie_neg3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dssm DNN Model模型训练\n",
    "def loss_function(user, movie_pos, movie_neg1,  movie_neg2, movie_neg3):\n",
    "    d_n = torch.exp(1.2 * torch.sum(user*movie_pos, 1))\n",
    "    d_n1 = torch.exp(1.2 * torch.sum(user*movie_neg1, 1))\n",
    "    d_n2 = torch.exp(1.2 * torch.sum(user*movie_neg2, 1))\n",
    "    d_n3 = torch.exp(1.2 * torch.sum(user*movie_neg3, 1))\n",
    "    p = torch.sum(-torch.log(d_n/(d_n + d_n1 + d_n2 + d_n3)))\n",
    "    return p\n",
    "    \n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epoch, loss_function, optimizer, path, early_stop):\n",
    "    \"\"\"\n",
    "    pytorch 模型训练通用代码\n",
    "    :param model: pytorch 模型\n",
    "    :param train_loader: dataloader, 训练数据\n",
    "    :param val_loader: dataloader, 验证数据\n",
    "    :param epoch: int, 训练迭代次数\n",
    "    :param loss_function: 优化损失函数\n",
    "    :param optimizer: pytorch优化器\n",
    "    :param path: save path\n",
    "    :param early_stop: int, 提前停止步数\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # 是否使用GPU\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 多少步内验证集的loss没有变小就提前停止\n",
    "    patience, eval_loss = 0, 0\n",
    "    \n",
    "    # 训练\n",
    "    for i in range(epoch):\n",
    "        total_loss, count = 0, 0\n",
    "        for idx, x in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            x = x.to(device)\n",
    "            u, m_p, m_n1, m_n2, m_n3 = model(x)\n",
    "            loss = loss_function(u, m_p, m_n1, m_n2, m_n3)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss)\n",
    "            count += 1\n",
    "            \n",
    "#         train_auc = roc_auc_score(np.array(y_true), np.array(y_pred))\n",
    "        torch.save(model, path.format(i+1))\n",
    "        print(\"Epoch %d train loss is %.3f\" % (i+1, total_loss / count))\n",
    "    \n",
    "        # 验证\n",
    "        total_eval_loss = 0\n",
    "        model.eval()\n",
    "        count_eval = 0\n",
    "        for idx, x in tqdm(enumerate(val_loader), total=len(val_loader)):\n",
    "            x = x.to(device)\n",
    "            u, m_p, m_n1, m_n2, m_n3 = model(x)\n",
    "            loss = loss_function(u, m_p, m_n1, m_n2, m_n3)\n",
    "            total_eval_loss += float(loss)\n",
    "            count_eval += 1\n",
    "        print(\"Epoch %d val loss is %.3f\" % (i+1, total_eval_loss / count_eval))\n",
    "        \n",
    "        # 提前停止策略\n",
    "        if i == 0:\n",
    "            eval_loss = total_eval_loss / count_eval\n",
    "        else:\n",
    "            if total_eval_loss / count_eval < eval_loss:\n",
    "                eval_loss = total_eval_loss / count_eval\n",
    "            else:\n",
    "                if patience < early_stop:\n",
    "                    patience += 1\n",
    "                else:\n",
    "                    print(\"val loss is not decrease in %d epoch and break training\" % patience)\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:47<00:00, 131.80it/s]\n",
      "  2%|▏         | 36/1558 [00:00<00:04, 350.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss is 126.849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1558/1558 [00:04<00:00, 350.93it/s]\n",
      "  0%|          | 3/6231 [00:00<03:36, 28.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 val loss is 119.830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:47<00:00, 129.93it/s]\n",
      "  2%|▏         | 35/1558 [00:00<00:04, 345.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss is 118.201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1558/1558 [00:04<00:00, 345.76it/s]\n",
      "  0%|          | 2/6231 [00:00<05:33, 18.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 val loss is 117.911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:47<00:00, 132.03it/s]\n",
      "  2%|▏         | 35/1558 [00:00<00:04, 347.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train loss is 116.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1558/1558 [00:04<00:00, 332.72it/s]\n",
      "  0%|          | 2/6231 [00:00<05:15, 19.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 val loss is 116.403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:46<00:00, 133.76it/s]\n",
      "  2%|▏         | 32/1558 [00:00<00:04, 313.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train loss is 116.155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1558/1558 [00:04<00:00, 315.11it/s]\n",
      "  0%|          | 2/6231 [00:00<05:22, 19.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 val loss is 121.150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:46<00:00, 133.98it/s]\n",
      "  2%|▏         | 35/1558 [00:00<00:04, 346.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train loss is 115.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1558/1558 [00:04<00:00, 347.74it/s]\n",
      "  0%|          | 3/6231 [00:00<03:29, 29.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 val loss is 116.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:47<00:00, 130.97it/s]\n",
      "  2%|▏         | 35/1558 [00:00<00:04, 348.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train loss is 115.580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1558/1558 [00:04<00:00, 351.10it/s]\n",
      "  0%|          | 3/6231 [00:00<03:36, 28.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 val loss is 115.408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:47<00:00, 130.32it/s]\n",
      "  2%|▏         | 35/1558 [00:00<00:04, 346.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train loss is 115.358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1558/1558 [00:04<00:00, 342.72it/s]\n",
      "  0%|          | 3/6231 [00:00<03:38, 28.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 val loss is 115.798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:47<00:00, 130.46it/s]\n",
      "  2%|▏         | 35/1558 [00:00<00:04, 346.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train loss is 115.267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1558/1558 [00:04<00:00, 350.04it/s]\n",
      "  0%|          | 3/6231 [00:00<03:45, 27.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 val loss is 115.401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:47<00:00, 130.14it/s]\n",
      "  2%|▏         | 35/1558 [00:00<00:04, 343.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train loss is 115.138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1558/1558 [00:04<00:00, 327.42it/s]\n",
      "  0%|          | 7/6231 [00:00<01:29, 69.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 val loss is 115.170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:47<00:00, 132.51it/s]\n",
      "  2%|▏         | 35/1558 [00:00<00:04, 346.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train loss is 115.110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1558/1558 [00:04<00:00, 329.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 val loss is 115.564\n",
      "val loss is not decrease in 3 epoch and break training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inp_user = 128\n",
    "inp_movie = 128\n",
    "out = 64\n",
    "input_user_categorical_feature = {0: (6040, 128)}\n",
    "input_movie_categorical_feature =  {0: (3883, 128)}\n",
    "hidden_layers = [128, 64]\n",
    "dropouts = [0.5, 0.5, 0.5]\n",
    "batch_norm = False\n",
    "\n",
    "model = DNNModel(inp_user, inp_movie, out, input_user_categorical_feature, input_movie_categorical_feature, \n",
    "                 hidden_layers, dropouts, batch_norm)\n",
    "\n",
    "epoch = 20\n",
    "early_stop = 3\n",
    "learn_rate = 0.005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "path = 'model/model_{}.pth'\n",
    "train_model(model, train_loader, test_loader, epoch, loss_function, optimizer, path, early_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3348/3348 [00:00<00:00, 10164.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall is 0.052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "user['movie_pos'] = 1\n",
    "user['movie_neg1'] = 2\n",
    "user['movie_neg2'] = 2\n",
    "user['movie_neg3'] = 2\n",
    "test_x = user[['user', 'movie_pos', 'movie_neg1', 'movie_neg2', 'movie_neg3']].values\n",
    "x = torch.from_numpy(test_x).cuda()\n",
    "user_embed, _, _, _, _ = model(x)\n",
    "\n",
    "movie['user'] = 1\n",
    "movie['movie_neg1'] = 1\n",
    "movie['movie_neg2'] = 1\n",
    "movie['movie_neg3'] = 1\n",
    "test_x = movie[['user', 'movie', 'movie_neg1', 'movie_neg2', 'movie_neg3']].values\n",
    "x = torch.from_numpy(test_x).cuda()\n",
    "_, movie_embed, _ ,_, _= model(x)\n",
    "\n",
    "movie_embed = movie_embed.cpu().detach().numpy()\n",
    "user_embed = user_embed.cpu().detach().numpy()\n",
    "\n",
    "# embedding 维度\n",
    "d = 64\n",
    "nlist = 10\n",
    "index = faiss.IndexFlatL2(d)\n",
    "# index = faiss.IndexIVFFlat(article_quantizer, d, nlist, faiss.METRIC_L2)\n",
    "# index.train(movie_embed)\n",
    "index.add(movie_embed)\n",
    "\n",
    "user_movie_dict_val = dict()\n",
    "for idx, rows in tqdm(val_data.iterrows(), total=len(val_data)):\n",
    "    u = rows['user']\n",
    "    m = rows['movie']\n",
    "    if u not in user_movie_dict_val:\n",
    "        user_movie_dict_val[u] = [m]\n",
    "    else:\n",
    "         user_movie_dict_val[u].append(m)\n",
    "            \n",
    "D, I = index.search(user_embed[list(val_data['user'].unique())], 50)\n",
    "\n",
    "# 召回率计算\n",
    "hits, total = 0, 0\n",
    "for uid, rec_list in zip(list(val_data['user'].unique()), I):\n",
    "    hits += len(set(rec_list)&set(user_movie_dict_val[uid]))\n",
    "    total += len(user_movie_dict_val[uid])\n",
    "print(\"recall is %.3f\" % (hits/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 添加user 和 movie相关特征\n",
    "之前的方案仅仅考虑了用户的ID和电影的ID特征，其他跟用户和电影相关的特征是完全没有考虑进来的，添加用户和电影相关的特征理论上是可以提升整个预测的性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996861/996861 [08:40<00:00, 1914.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# 随机负采样\n",
    "sample_list = list(train_data['movie'].unique())\n",
    "data = list()\n",
    "for idx, rows in tqdm(train_data.iterrows(), total=len(train_data)):\n",
    "    use = rows['user']\n",
    "    mov = rows['movie']\n",
    "    data.append([use, mov, 1])\n",
    "    for m in np.random.choice(sample_list, 3):\n",
    "        data.append([use, m, 0])\n",
    "data = pd.DataFrame(data, columns=['user', 'movie', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并用户特征和电影特征\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "user['gender'] = le.fit_transform(user['gender'])\n",
    "user['age'] = le.fit_transform(user['age'])\n",
    "user['occupation'] = le.fit_transform(user['occupation'])\n",
    "\n",
    "data = pd.merge(data, user[['user', 'gender', 'age', 'occupation']], how='left', on='user')\n",
    "\n",
    "genres = list()\n",
    "tmp = movie['genres'].apply(lambda x: x.split('|'))\n",
    "for l in tmp.tolist():\n",
    "    genres += l\n",
    "    \n",
    "genres_dict = dict()\n",
    "for idx, g in  enumerate(list(set(genres))):\n",
    "    genres_dict[g] = idx + 1\n",
    "    \n",
    "movie['genres'] = tmp.apply(lambda x: [genres_dict[i] for i in x])\n",
    "\n",
    "data = pd.merge(data, movie[['movie', 'genres']], how='left', on='movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>tag</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1176</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1360</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[13, 12, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[4, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1865</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>655</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[2, 15, 12]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  movie  tag  gender  age  occupation        genres\n",
       "0     0   1176    1       0    0          10           [4]\n",
       "1     0   1360    0       0    0          10  [13, 12, 10]\n",
       "2     0    298    0       0    0          10       [4, 10]\n",
       "3     0   1865    0       0    0          10          [13]\n",
       "4     0    655    1       0    0          10   [2, 15, 12]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.x = data[0]\n",
    "        self.y = data[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "        data = (x, y)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "# 训练集验证集随机分割\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=2021)\n",
    "train_x = train_df[['user', 'gender', 'age', 'occupation', 'movie']].values\n",
    "train_y = train_df['tag'].values\n",
    "test_x = test_df[['user', 'gender', 'age', 'occupation', 'movie']].values\n",
    "test_y = test_df['tag'].values\n",
    "\n",
    "# 构造dataloader\n",
    "train_dataset = trainset((train_x, train_y))\n",
    "test_dataset = trainset((test_x, test_y))\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNModel(nn.Module):\n",
    "    def __init__(self, inp_user, inp_movie, out, input_user_categorical_feature, input_movie_categorical_feature, \n",
    "                 hidden_layers, dropouts, batch_norm):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.user_embed = nn.Embedding(input_user_categorical_feature[0][0], input_user_categorical_feature[0][1])\n",
    "        self.gender_embed = nn.Embedding(input_user_categorical_feature[1][0], input_user_categorical_feature[1][1])\n",
    "        self.age_embed = nn.Embedding(input_user_categorical_feature[2][0], input_user_categorical_feature[2][1])\n",
    "        self.occupation_embed = nn.Embedding(input_user_categorical_feature[3][0], input_user_categorical_feature[3][1])\n",
    "        self.movie_embed = nn.Embedding(input_movie_categorical_feature[0][0], input_movie_categorical_feature[0][1])\n",
    "#         self.genres_embed = nn.Embedding(input_movie_categorical_feature[1][0], input_movie_categorical_feature[1][1])\n",
    "        \n",
    "        self.user_dnn = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "#             nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        \n",
    "        self.movie_dnn = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "#             nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        u = self.user_embed(x[:, 0])\n",
    "        g = self.gender_embed(x[:, 1])\n",
    "        a = self.age_embed(x[:, 2])\n",
    "        oc = self.occupation_embed(x[:, 3])\n",
    "        m = self.movie_embed(x[:, 4])\n",
    "        \n",
    "        u = torch.cat([u, g, a, oc], -1)\n",
    "        u = self.user_dnn(u)\n",
    "        m = self.movie_dnn(m)\n",
    "        u = u/torch.sum(u*u, 1).view(-1,1)\n",
    "        m = m/torch.sum(m*m, 1).view(-1,1)\n",
    "        return u, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epoch, loss_function, optimizer, path, early_stop):\n",
    "    \"\"\"\n",
    "    pytorch 模型训练通用代码\n",
    "    :param model: pytorch 模型\n",
    "    :param train_loader: dataloader, 训练数据\n",
    "    :param val_loader: dataloader, 验证数据\n",
    "    :param epoch: int, 训练迭代次数\n",
    "    :param loss_function: 优化损失函数\n",
    "    :param optimizer: pytorch优化器\n",
    "    :param path: save path\n",
    "    :param early_stop: int, 提前停止步数\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # 是否使用GPU\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 多少步内验证集的loss没有变小就提前停止\n",
    "    patience, eval_loss = 0, 0\n",
    "    \n",
    "    # 训练\n",
    "    for i in range(epoch):\n",
    "        total_loss, count = 0, 0\n",
    "        y_pred = list()\n",
    "        y_true = list()\n",
    "        for idx, (x, y) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            x, y = x.to(device), y.to(device) \n",
    "            u, m = model(x)\n",
    "            predict = torch.sigmoid(torch.sum(u*m, 1))\n",
    "            y_pred.extend(predict.cpu().detach().numpy())\n",
    "            y_true.extend(y.cpu().detach().numpy())\n",
    "            loss = loss_function(predict, y.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss)\n",
    "            count += 1\n",
    "            \n",
    "        train_auc = roc_auc_score(np.array(y_true), np.array(y_pred))\n",
    "        torch.save(model, path.format(i+1))\n",
    "        print(\"Epoch %d train loss is %.3f and train auc is %.3f\" % (i+1, total_loss / count, train_auc))\n",
    "    \n",
    "        # 验证\n",
    "        total_eval_loss = 0\n",
    "        model.eval()\n",
    "        count_eval = 0\n",
    "        val_y_pred = list()\n",
    "        val_true = list()\n",
    "        for idx, (x, y) in tqdm(enumerate(val_loader), total=len(val_loader)):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            u, m = model(x)\n",
    "            predict = torch.sigmoid(torch.sum(u*m, 1))\n",
    "            val_y_pred.extend(predict.cpu().detach().numpy())\n",
    "            val_true.extend(y.cpu().detach().numpy())\n",
    "            loss = loss_function(predict, y.float())\n",
    "            total_eval_loss += float(loss)\n",
    "            count_eval += 1\n",
    "        val_auc = roc_auc_score(np.array(y_true), np.array(y_pred))\n",
    "        print(\"Epoch %d val loss is %.3fand train auc is %.3f\" % (i+1, total_eval_loss / count_eval, val_auc))\n",
    "        \n",
    "        # 提前停止策略\n",
    "        if i == 0:\n",
    "            eval_loss = total_eval_loss / count_eval\n",
    "        else:\n",
    "            if total_eval_loss / count_eval < eval_loss:\n",
    "                eval_loss = total_eval_loss / count_eval\n",
    "            else:\n",
    "                if patience < early_stop:\n",
    "                    patience += 1\n",
    "                else:\n",
    "                    print(\"val loss is not decrease in %d epoch and break training\" % patience)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24922/24922 [03:27<00:00, 120.34it/s]\n",
      "  1%|          | 41/6231 [00:00<00:15, 407.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss is 0.700 and train auc is 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:19<00:00, 313.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 val loss is 0.692and train auc is 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24922/24922 [03:10<00:00, 131.15it/s]\n",
      "  1%|          | 44/6231 [00:00<00:14, 435.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss is 0.693 and train auc is 0.744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:19<00:00, 325.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 val loss is 0.692and train auc is 0.744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24922/24922 [02:40<00:00, 155.62it/s]\n",
      "  1%|          | 45/6231 [00:00<00:14, 441.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train loss is 0.692 and train auc is 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:14<00:00, 430.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 val loss is 0.691and train auc is 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24922/24922 [02:37<00:00, 158.64it/s]\n",
      "  1%|          | 44/6231 [00:00<00:14, 432.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train loss is 0.691 and train auc is 0.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:14<00:00, 435.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 val loss is 0.690and train auc is 0.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24922/24922 [02:35<00:00, 160.22it/s]\n",
      "  1%|          | 44/6231 [00:00<00:14, 432.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train loss is 0.691 and train auc is 0.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:14<00:00, 415.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 val loss is 0.690and train auc is 0.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24922/24922 [02:34<00:00, 161.64it/s]\n",
      "  1%|          | 43/6231 [00:00<00:14, 426.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train loss is 0.691 and train auc is 0.742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:14<00:00, 431.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 val loss is 0.691and train auc is 0.742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24922/24922 [02:34<00:00, 161.24it/s]\n",
      "  1%|          | 43/6231 [00:00<00:14, 427.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train loss is 0.691 and train auc is 0.742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:14<00:00, 427.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 val loss is 0.690and train auc is 0.742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24922/24922 [02:35<00:00, 160.77it/s]\n",
      "  1%|          | 34/6231 [00:00<00:18, 328.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train loss is 0.692 and train auc is 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:14<00:00, 417.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 val loss is 0.690and train auc is 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24922/24922 [02:34<00:00, 161.28it/s]\n",
      "  1%|          | 42/6231 [00:00<00:14, 413.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train loss is 0.691 and train auc is 0.740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6231/6231 [00:14<00:00, 423.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 val loss is 0.693and train auc is 0.740\n",
      "val loss is not decrease in 3 epoch and break training\n"
     ]
    }
   ],
   "source": [
    "# 模型初始化\n",
    "inp_user = 128\n",
    "inp_movie = 128\n",
    "out = 64\n",
    "input_user_categorical_feature = {0: (6040, 128), 1: (2, 128), 2: (7, 128), 3: (21, 128)}\n",
    "input_movie_categorical_feature =  {0: (3883, 128), 1:(18, 128)}\n",
    "hidden_layers = [128, 64]\n",
    "dropouts = [0.5, 0.5, 0.5]\n",
    "batch_norm = False\n",
    "\n",
    "model = DNNModel(inp_user, inp_movie, out, input_user_categorical_feature, input_movie_categorical_feature, \n",
    "                 hidden_layers, dropouts, batch_norm)\n",
    "\n",
    "# 模型训练\n",
    "epoch = 20\n",
    "loss_function = F.binary_cross_entropy_with_logits\n",
    "early_stop = 3\n",
    "learn_rate = 0.005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "path = 'model/model_{}.pth'\n",
    "\n",
    "train_model(model, train_loader, test_loader, epoch, loss_function, optimizer, path, early_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3348/3348 [00:00<00:00, 8214.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall is 0.063\n"
     ]
    }
   ],
   "source": [
    "# 结果验证\n",
    "model.eval()\n",
    "user['movie'] = 1\n",
    "test_x = user[['user', 'gender', 'age', 'occupation', 'movie']].values\n",
    "x = torch.from_numpy(test_x).cuda()\n",
    "user_embed, _ = model(x)\n",
    "\n",
    "movie['user'] = 1\n",
    "movie['gender'] = 1\n",
    "movie['age'] = 1\n",
    "movie['occupation'] = 1\n",
    "test_x = movie[['user', 'gender', 'age', 'occupation','movie']].values\n",
    "x = torch.from_numpy(test_x).cuda()\n",
    "_, movie_embed = model(x)\n",
    "\n",
    "movie_embed = movie_embed.cpu().detach().numpy()\n",
    "user_embed = user_embed.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# faiss索引构建\n",
    "d = 64\n",
    "nlist = 10\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(movie_embed)\n",
    "\n",
    "# 验证集数据字典化\n",
    "user_movie_dict_val = dict()\n",
    "for idx, rows in tqdm(val_data.iterrows(), total=len(val_data)):\n",
    "    u = rows['user']\n",
    "    m = rows['movie']\n",
    "    if u not in user_movie_dict_val:\n",
    "        user_movie_dict_val[u] = [m]\n",
    "    else:\n",
    "         user_movie_dict_val[u].append(m)\n",
    "            \n",
    "# 用户推荐结果索引           \n",
    "D, I = index.search(user_embed[list(val_data['user'].unique())], 50)\n",
    "\n",
    "# 召回率计算\n",
    "hits, total = 0, 0\n",
    "for uid, rec_list in zip(list(val_data['user'].unique()), I):\n",
    "    hits += len(set(rec_list)&set(user_movie_dict_val[uid]))\n",
    "    total += len(user_movie_dict_val[uid])\n",
    "print(\"recall is %.3f\" % (hits/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "目前实验效果较好的方案是基于0，1 label的随机负采样方案和基于triplet loss的正负样本距离方案，考虑加入用户相关的属性特征融合到用户向量中目前效果不是特别理想后续会尝试优化，目前实验的效果稳定性还有一定的优化空间参数也有一定的调优空间，具体调惨和初始化方案可以深入尝试。希望通过上述的方案能直观感受到各种深度召回方案之间的差距和问题，实践结合理论才能提升自我。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
